{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import scipy\n",
    "import multiprocessing as mp\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(nn.Module):\n",
    "    def __init__(self, n_in, n_out, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.n_in  = n_in\n",
    "        self.n_out = n_out\n",
    "        self.linear = nn.Linear(n_in,  n_out)\n",
    "    def forward(self, x, adj,cluster_labelt):\n",
    "        out = self.linear(x)\n",
    "        return F.elu(torch.spmm(adj, torch.mul(torch.mul(cluster_label.T,x),cluster_label)))\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, layers, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "        self.layers = layers\n",
    "        self.nhid = nhid\n",
    "        self.gcs = nn.ModuleList()\n",
    "        self.gcs.append(GraphConvolution(nfeat,  nhid))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        for i in range(layers-1):\n",
    "            self.gcs.append(GraphConvolution(nhid,  nhid))\n",
    "    def forward(self, x, adjs,cluster_label):\n",
    "        '''\n",
    "            The difference here with the original GCN implementation is that\n",
    "            we will receive different adjacency matrix for different layer.\n",
    "        '''\n",
    "        for idx in range(len(self.gcs)):\n",
    "            x = self.dropout(self.gcs[idx](x, adjs[idx],cluster_label))\n",
    "        return x\n",
    "    \n",
    "class attenGCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, layers, dropout):\n",
    "        super(denseGCN, self).__init__()\n",
    "        self.layers = layers\n",
    "        self.nhid = nhid\n",
    "        self.gcs = nn.ModuleList()\n",
    "        self.gcs.append(GraphConvolution(nfeat,  nhid))\n",
    "        self.atten = nn.Parameter(torch.ones(size=(nhid, 1)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.leakyrelu = nn.LeakyReLU(0.1)\n",
    "        \n",
    "        for i in range(layers-1):\n",
    "            self.gcs.append(GraphConvolution(nhid,  nhid))\n",
    "            \n",
    "    def forward(self, x, adjs,cluster_label):\n",
    "        emb = []      \n",
    "        for idx in range(len(self.gcs)):\n",
    "            if idx == 0:\n",
    "                emb += [self.dropout(self.gcs[idx](x, adjs[idx],cluster_label))]\n",
    "            else:\n",
    "                emb += [self.dropout(self.gcs[idx](emb[-1], adjs[idx],cluster_label))]\n",
    "            \n",
    "            if len(self.gcs) > 1:\n",
    "                all_emb = torch.stack(emb)\n",
    "                e = self.leakyrelu(torch.matmul(all_emb, self.atten).squeeze(2))              \n",
    "                attention = F.softmax(e, dim=0).unsqueeze(2).repeat(1,1,self.nhid)\n",
    "                output = torch.sum(attention*all_emb, dim=0)\n",
    "            else:\n",
    "                output = emb[-1]\n",
    "            \n",
    "        return output\n",
    "\n",
    "class SuGCN(nn.Module):\n",
    "    def __init__(self, encoder, num_classes, dropout, inp):\n",
    "        super(SuGCN, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear  = nn.Linear(self.encoder.nhid, num_classes)\n",
    "    def forward(self, feat, adjs,cluster_label):\n",
    "        x = self.encoder(feat, adjs,cluster_label)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "cuda = 0\n",
    "dataset = \"cora\"\n",
    "sample_method = \"full\"\n",
    "samp_num = 64\n",
    "nhid = 256\n",
    "epoch_num = 100\n",
    "pool_num = 10\n",
    "batch_num = 10\n",
    "n_layers = 5\n",
    "n_iters = 1\n",
    "n_stops = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_sampler(seed, batch_nodes, samp_num_list, num_nodes, lap_matrix, depth):\n",
    "    mx = sparse_mx_to_torch_sparse_tensor(lap_matrix)\n",
    "    return [mx for i in range(depth)], np.arange(num_nodes), batch_nodes\n",
    "def prepare_data(pool, sampler, process_ids, train_nodes, valid_nodes, samp_num_list, num_nodes, lap_matrix, depth):\n",
    "    jobs = []\n",
    "    for _ in process_ids:\n",
    "        idx = torch.randperm(len(train_nodes))[:batch_size]\n",
    "        batch_nodes = train_nodes[idx]\n",
    "        p = pool.apply_async(sampler, args=(np.random.randint(2**32 - 1), batch_nodes,samp_num_list, num_nodes, lap_matrix, depth))\n",
    "        jobs.append(p)\n",
    "    idx = torch.randperm(len(valid_nodes))[:batch_size]\n",
    "    batch_nodes = valid_nodes[idx]\n",
    "    p = pool.apply_async(sampler, args=(np.random.randint(2**32 - 1), batch_nodes, samp_num_list * 20, num_nodes, lap_matrix, depth))\n",
    "    jobs.append(p)\n",
    "    return jobs\n",
    "def package_mxl(mxl, device):\n",
    "    return [torch.sparse.FloatTensor(mx[0], mx[1], mx[2]).to(device) for mx in mxl]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cora full\n"
     ]
    }
   ],
   "source": [
    "if cuda != -1:\n",
    "    device = torch.device(\"cuda:\" + str(cuda))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "print(dataset, sample_method)\n",
    "edges, labels, feat_data, num_classes, train_nodes, valid_nodes, test_nodes = load_data(dataset)\n",
    "\n",
    "adj_matrix = get_adj(edges, feat_data.shape[0])\n",
    "sc = SpectralClustering(num_classes, affinity='precomputed', random_state=0,n_init=100,assign_labels='discretize')\n",
    "# Tune: adj_matrix -> lap_matrix??\n",
    "sc.fit(scipy.sparse.csr_matrix.todense(adj_matrix))\n",
    "cluster_label_mat = np.array([1 * (sc.labels_ == 0)])\n",
    "\n",
    "for i in range(1, num_classes):\n",
    "    cluster_label_mat = np.concatenate((cluster_label_mat, np.array([1 * (sc.labels_ == i)])), axis = 0)\n",
    "\n",
    "lap_matrix = row_normalize(adj_matrix + sp.eye(adj_matrix.shape[0]))\n",
    "if type(feat_data) == scipy.sparse.lil.lil_matrix:\n",
    "    feat_data = torch.FloatTensor(feat_data.todense()).to(device) \n",
    "else:\n",
    "    feat_data = torch.FloatTensor(feat_data).to(device)\n",
    "labels    = torch.LongTensor(labels).to(device)\n",
    "\n",
    "if type(cluster_label_mat) == scipy.sparse.lil.lil_matrix:\n",
    "    cluster_label_mat = torch.FloatTensor(cluster_label_mat.todense()).to(device) \n",
    "else:\n",
    "    cluster_label_mat = torch.FloatTensor(cluster_label_mat).to(device)\n",
    "\n",
    "sampler = default_sampler\n",
    "GCN = attenGCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlayers = range(10)\n",
    "test_acc_GCN = []\n",
    "for n_layer in nlayers:\n",
    "    n_layers = n_layer + 1\n",
    "    process_ids = np.arange(batch_num)\n",
    "    samp_num_list = np.array([samp_num, samp_num, samp_num, samp_num, samp_num])\n",
    "\n",
    "    pool = mp.Pool(pool_num)\n",
    "    jobs = prepare_data(pool, sampler, process_ids, train_nodes, valid_nodes, samp_num_list, len(feat_data), lap_matrix, n_layers)\n",
    "\n",
    "    all_res = []\n",
    "    for oiter in range(5):\n",
    "        encoder = GCN(nfeat = feat_data.shape[1], nhid=nhid, layers=n_layers, dropout = 0.2).to(device)\n",
    "        susage  = SuGCN(encoder = encoder, num_classes=num_classes, dropout=0.5, inp = feat_data.shape[1])\n",
    "        susage.to(device)\n",
    "\n",
    "        optimizer = optim.Adam(filter(lambda p : p.requires_grad, susage.parameters()))\n",
    "        best_val = 0\n",
    "        best_tst = -1\n",
    "        cnt = 0\n",
    "        times = []\n",
    "        res   = []\n",
    "        print('-' * 10)\n",
    "        for epoch in np.arange(epoch_num):\n",
    "            susage.train()\n",
    "            train_losses = []\n",
    "            train_data = [job.get() for job in jobs[:-1]]\n",
    "            valid_data = jobs[-1].get()\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "            pool = mp.Pool(pool_num)\n",
    "            '''\n",
    "                Use CPU-GPU cooperation to reduce the overhead for sampling. (conduct sampling while training)\n",
    "            '''\n",
    "            jobs = prepare_data(pool, sampler, process_ids, train_nodes, valid_nodes, samp_num_list, len(feat_data), lap_matrix, n_layers)\n",
    "            for _iter in range(n_iters):\n",
    "                for adjs, input_nodes, output_nodes in train_data:    \n",
    "                    adjs = package_mxl(adjs, device)\n",
    "                    optimizer.zero_grad()\n",
    "                    t1 = time.time()\n",
    "                    susage.train()\n",
    "                    output = susage.forward(feat_data[input_nodes], adjs)\n",
    "                    if sample_method == 'full':\n",
    "                        output = output[output_nodes]\n",
    "                    loss_train = F.cross_entropy(output, labels[output_nodes])\n",
    "                    loss_train.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(susage.parameters(), 0.2)\n",
    "                    optimizer.step()\n",
    "                    times += [time.time() - t1]\n",
    "                    train_losses += [loss_train.detach().tolist()]\n",
    "                    del loss_train\n",
    "            susage.eval()\n",
    "            adjs, input_nodes, output_nodes = valid_data\n",
    "            adjs = package_mxl(adjs, device)\n",
    "            output = susage.forward(feat_data[input_nodes], adjs,cluster_label_mat)\n",
    "            if sample_method == 'full':\n",
    "                output = output[output_nodes]\n",
    "            loss_valid = F.cross_entropy(output, labels[output_nodes]).detach().tolist()\n",
    "            valid_f1 = f1_score(output.argmax(dim=1).cpu(), labels[output_nodes].cpu(), average='micro')\n",
    "            print((\"Epoch: %d (%.1fs) Train Loss: %.2f    Valid Loss: %.2f Valid F1: %.3f\") %                   (epoch, np.sum(times), np.average(train_losses), loss_valid, valid_f1))\n",
    "            if valid_f1 > best_val + 1e-2:\n",
    "                best_val = valid_f1\n",
    "                torch.save(susage, './save/best_model.pt')\n",
    "                cnt = 0\n",
    "            else:\n",
    "                cnt += 1\n",
    "            if cnt == n_stops // batch_num:\n",
    "                break\n",
    "        best_model = torch.load('./save/best_model.pt')\n",
    "        best_model.eval()\n",
    "        test_f1s = []\n",
    "        for b in np.arange(len(test_nodes) // batch_size):\n",
    "            batch_nodes = test_nodes[b * batch_size : (b+1) * batch_size]\n",
    "            adjs, input_nodes, output_nodes = default_sampler(np.random.randint(2**32 - 1), batch_nodes,\n",
    "                                        samp_num_list * 20, len(feat_data), lap_matrix, n_layers)\n",
    "            adjs = package_mxl(adjs, device)\n",
    "            output = best_model.forward(feat_data[input_nodes], adjs,cluster_label_mat)[output_nodes]\n",
    "            test_f1 = f1_score(output.argmax(dim=1).cpu(), labels[output_nodes].cpu(), average='micro')\n",
    "            test_f1s += [test_f1]\n",
    "        print('Number of Layers: %d, Iteration: %d, Test F1: %.3f' % (n_layers, oiter, np.average(test_f1s)))\n",
    "    test_acc_GCN += [np.average(test_f1s)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_acc_atten)\n",
    "plt.plot(test_acc_GCN)\n",
    "plt.legend([\"with attention\", \"without attention\"], fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_aws_neuron_pytorch_p36)",
   "language": "python",
   "name": "conda_aws_neuron_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
